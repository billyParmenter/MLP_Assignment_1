{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Group-5 Machine Learning Programming (PROG8245)**\n",
    "# **Final Project Model Development**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import the Required Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dwara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dwara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dwara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngoc Pham, a 83 yr old Vietnamese man, was one...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I’m making the following announcement and form...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“The most aggressive field operation in Califo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you Supervisor @AaronPeskin for endorsin...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#tbt sharing a meal w/my friend @narendramodi ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>Hey @united, I’m about to board one of your fl...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>OFFICIAL WELCOME: @MikePenceVP let me be the f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>BREAKING: A Republican Member of the Pennsylva...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>Today, I'm joined by 8 of my colleagues in int...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>I'm introducing legislation to ban the use of ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweets sentiment\n",
       "0    Ngoc Pham, a 83 yr old Vietnamese man, was one...  negative\n",
       "1    I’m making the following announcement and form...  positive\n",
       "2    “The most aggressive field operation in Califo...  positive\n",
       "3    Thank you Supervisor @AaronPeskin for endorsin...  positive\n",
       "4    #tbt sharing a meal w/my friend @narendramodi ...  positive\n",
       "..                                                 ...       ...\n",
       "444  Hey @united, I’m about to board one of your fl...   neutral\n",
       "445  OFFICIAL WELCOME: @MikePenceVP let me be the f...  positive\n",
       "446  BREAKING: A Republican Member of the Pennsylva...  negative\n",
       "447  Today, I'm joined by 8 of my colleagues in int...  negative\n",
       "448  I'm introducing legislation to ban the use of ...  negative\n",
       "\n",
       "[449 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the tweets dataset\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\dwara\\OneDrive\\Desktop\\ML_Programming\\Labs\\MLP_Project\\MLP_Assignment_1\\data\\hugginoutput.csv\")\n",
    "df.columns = ['tweets', 'sentiment']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Basic Preprocessing of Tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ngoc pham 83 yr old vietnamese man one 2 asian...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>making following announcement formal apology t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aggressive field operation california sander a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank supervisor aaronpeskin endorsing senator...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tbt sharing meal friend narendramodi wishing l...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>hey united board one flight sure hope given fl...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>official welcome mikepencevp let first officia...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>breaking republican member pennsylvania house ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>today joined 8 colleague introducing walk with...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>introducing legislation ban use tear gas distr...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweets sentiment\n",
       "0    ngoc pham 83 yr old vietnamese man one 2 asian...  negative\n",
       "1    making following announcement formal apology t...  positive\n",
       "2    aggressive field operation california sander a...  positive\n",
       "3    thank supervisor aaronpeskin endorsing senator...  positive\n",
       "4    tbt sharing meal friend narendramodi wishing l...  positive\n",
       "..                                                 ...       ...\n",
       "444  hey united board one flight sure hope given fl...   neutral\n",
       "445  official welcome mikepencevp let first officia...  positive\n",
       "446  breaking republican member pennsylvania house ...  negative\n",
       "447  today joined 8 colleague introducing walk with...  negative\n",
       "448  introducing legislation ban use tear gas distr...  negative\n",
       "\n",
       "[449 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize NLP Preprocessing functions\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess the \"tweets\" column in place\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sentence = row['tweets']\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens if token.isalnum()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a preprocessed sentence\n",
    "    preprocessed_sentence = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    # Update the \"tweets\" column with preprocessed sentence\n",
    "    df.at[index, 'tweets'] = preprocessed_sentence\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Embeddings Creation using tfidf vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>10</th>\n",
       "      <th>100k</th>\n",
       "      <th>10am</th>\n",
       "      <th>11</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15th</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youngest</th>\n",
       "      <th>yr</th>\n",
       "      <th>zelenskyy</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ngoc pham 83 yr old vietnamese man one 2 asian...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.186308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.340949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>making following announcement formal apology t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aggressive field operation california sander a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank supervisor aaronpeskin endorsing senator...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tbt sharing meal friend narendramodi wishing l...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>hey united board one flight sure hope given fl...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>official welcome mikepencevp let first officia...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>breaking republican member pennsylvania house ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>today joined 8 colleague introducing walk with...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>introducing legislation ban use tear gas distr...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449 rows × 2245 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweets sentiment   10  100k   \n",
       "0    ngoc pham 83 yr old vietnamese man one 2 asian...  negative  0.0   0.0  \\\n",
       "1    making following announcement formal apology t...  positive  0.0   0.0   \n",
       "2    aggressive field operation california sander a...  positive  0.0   0.0   \n",
       "3    thank supervisor aaronpeskin endorsing senator...  positive  0.0   0.0   \n",
       "4    tbt sharing meal friend narendramodi wishing l...  positive  0.0   0.0   \n",
       "..                                                 ...       ...  ...   ...   \n",
       "444  hey united board one flight sure hope given fl...   neutral  0.0   0.0   \n",
       "445  official welcome mikepencevp let first officia...  positive  0.0   0.0   \n",
       "446  breaking republican member pennsylvania house ...  negative  0.0   0.0   \n",
       "447  today joined 8 colleague introducing walk with...  negative  0.0   0.0   \n",
       "448  introducing legislation ban use tear gas distr...  negative  0.0   0.0   \n",
       "\n",
       "     10am   11   13   14  15th   16  ...      year  yes  yesterday  yet  york   \n",
       "0     0.0  0.0  0.0  0.0   0.0  0.0  ...  0.000000  0.0   0.186308  0.0   0.0  \\\n",
       "1     0.0  0.0  0.0  0.0   0.0  0.0  ...  0.000000  0.0   0.000000  0.0   0.0   \n",
       "2     0.0  0.0  0.0  0.0   0.0  0.0  ...  0.000000  0.0   0.000000  0.0   0.0   \n",
       "3     0.0  0.0  0.0  0.0   0.0  0.0  ...  0.000000  0.0   0.000000  0.0   0.0   \n",
       "4     0.0  0.0  0.0  0.0   0.0  0.0  ...  0.000000  0.0   0.000000  0.0   0.0   \n",
       "..    ...  ...  ...  ...   ...  ...  ...       ...  ...        ...  ...   ...   \n",
       "444   0.0  0.0  0.0  0.0   0.0  0.0  ...  0.000000  0.0   0.000000  0.0   0.0   \n",
       "445   0.0  0.0  0.0  0.0   0.0  0.0  ...  0.000000  0.0   0.000000  0.0   0.0   \n",
       "446   0.0  0.0  0.0  0.0   0.0  0.0  ...  0.000000  0.0   0.000000  0.0   0.0   \n",
       "447   0.0  0.0  0.0  0.0   0.0  0.0  ...  0.137571  0.0   0.000000  0.0   0.0   \n",
       "448   0.0  0.0  0.0  0.0   0.0  0.0  ...  0.000000  0.0   0.000000  0.0   0.0   \n",
       "\n",
       "     young  youngest        yr  zelenskyy     zero  \n",
       "0      0.0       0.0  0.340949        0.0  0.00000  \n",
       "1      0.0       0.0  0.000000        0.0  0.00000  \n",
       "2      0.0       0.0  0.000000        0.0  0.00000  \n",
       "3      0.0       0.0  0.000000        0.0  0.00000  \n",
       "4      0.0       0.0  0.000000        0.0  0.00000  \n",
       "..     ...       ...       ...        ...      ...  \n",
       "444    0.0       0.0  0.000000        0.0  0.00000  \n",
       "445    0.0       0.0  0.000000        0.0  0.00000  \n",
       "446    0.0       0.0  0.000000        0.0  0.00000  \n",
       "447    0.0       0.0  0.000000        0.0  0.20922  \n",
       "448    0.0       0.0  0.000000        0.0  0.00000  \n",
       "\n",
       "[449 rows x 2245 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed tweets\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['tweets'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate the original DataFrame with the TF-IDF DataFrame\n",
    "\n",
    "tfidf_df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# Remove the sentence_embeddings column\n",
    "\n",
    "tfidf_df = tfidf_df.drop(columns=['sentence_embeddings'])\n",
    "\n",
    "# Display the final output dataframe with embeddings\n",
    "\n",
    "tfidf_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Embeddings creation using Word2Vec Skipgram Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_490</th>\n",
       "      <th>feature_491</th>\n",
       "      <th>feature_492</th>\n",
       "      <th>feature_493</th>\n",
       "      <th>feature_494</th>\n",
       "      <th>feature_495</th>\n",
       "      <th>feature_496</th>\n",
       "      <th>feature_497</th>\n",
       "      <th>feature_498</th>\n",
       "      <th>feature_499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ngoc pham 83 yr old vietnamese man one 2 asian...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.076744</td>\n",
       "      <td>0.030947</td>\n",
       "      <td>0.046186</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>-0.081016</td>\n",
       "      <td>-0.083532</td>\n",
       "      <td>-0.008621</td>\n",
       "      <td>0.120812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043578</td>\n",
       "      <td>-0.012962</td>\n",
       "      <td>0.054955</td>\n",
       "      <td>0.058670</td>\n",
       "      <td>0.016228</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>-0.011431</td>\n",
       "      <td>0.026834</td>\n",
       "      <td>-0.042793</td>\n",
       "      <td>0.018279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>making following announcement formal apology t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.076372</td>\n",
       "      <td>0.030456</td>\n",
       "      <td>0.045402</td>\n",
       "      <td>0.010186</td>\n",
       "      <td>-0.080341</td>\n",
       "      <td>-0.082916</td>\n",
       "      <td>-0.008668</td>\n",
       "      <td>0.119791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043976</td>\n",
       "      <td>-0.013322</td>\n",
       "      <td>0.054382</td>\n",
       "      <td>0.058117</td>\n",
       "      <td>0.016160</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>-0.011921</td>\n",
       "      <td>0.026654</td>\n",
       "      <td>-0.042991</td>\n",
       "      <td>0.017968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aggressive field operation california sander a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.076763</td>\n",
       "      <td>0.030808</td>\n",
       "      <td>0.045769</td>\n",
       "      <td>0.009861</td>\n",
       "      <td>-0.080629</td>\n",
       "      <td>-0.082838</td>\n",
       "      <td>-0.008533</td>\n",
       "      <td>0.120222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044042</td>\n",
       "      <td>-0.012846</td>\n",
       "      <td>0.055301</td>\n",
       "      <td>0.058937</td>\n",
       "      <td>0.016421</td>\n",
       "      <td>0.008672</td>\n",
       "      <td>-0.011221</td>\n",
       "      <td>0.026781</td>\n",
       "      <td>-0.043898</td>\n",
       "      <td>0.018478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank supervisor aaronpeskin endorsing senator...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.075840</td>\n",
       "      <td>0.030218</td>\n",
       "      <td>0.045851</td>\n",
       "      <td>0.009287</td>\n",
       "      <td>-0.080463</td>\n",
       "      <td>-0.081054</td>\n",
       "      <td>-0.007640</td>\n",
       "      <td>0.117877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046128</td>\n",
       "      <td>-0.014048</td>\n",
       "      <td>0.057207</td>\n",
       "      <td>0.060641</td>\n",
       "      <td>0.015682</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>-0.013825</td>\n",
       "      <td>0.027092</td>\n",
       "      <td>-0.045812</td>\n",
       "      <td>0.019946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tbt sharing meal friend narendramodi wishing l...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.076430</td>\n",
       "      <td>0.030484</td>\n",
       "      <td>0.045450</td>\n",
       "      <td>0.010247</td>\n",
       "      <td>-0.080818</td>\n",
       "      <td>-0.082362</td>\n",
       "      <td>-0.008108</td>\n",
       "      <td>0.119774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043510</td>\n",
       "      <td>-0.013013</td>\n",
       "      <td>0.054999</td>\n",
       "      <td>0.058280</td>\n",
       "      <td>0.015952</td>\n",
       "      <td>0.008525</td>\n",
       "      <td>-0.011212</td>\n",
       "      <td>0.026109</td>\n",
       "      <td>-0.043871</td>\n",
       "      <td>0.018485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>hey united board one flight sure hope given fl...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.077678</td>\n",
       "      <td>0.031790</td>\n",
       "      <td>0.046427</td>\n",
       "      <td>0.010435</td>\n",
       "      <td>-0.081350</td>\n",
       "      <td>-0.083798</td>\n",
       "      <td>-0.008335</td>\n",
       "      <td>0.122458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043204</td>\n",
       "      <td>-0.012310</td>\n",
       "      <td>0.055704</td>\n",
       "      <td>0.058883</td>\n",
       "      <td>0.016722</td>\n",
       "      <td>0.007232</td>\n",
       "      <td>-0.010740</td>\n",
       "      <td>0.026866</td>\n",
       "      <td>-0.042810</td>\n",
       "      <td>0.018456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>official welcome mikepencevp let first officia...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.075786</td>\n",
       "      <td>0.030676</td>\n",
       "      <td>0.045305</td>\n",
       "      <td>0.009415</td>\n",
       "      <td>-0.080681</td>\n",
       "      <td>-0.082691</td>\n",
       "      <td>-0.008398</td>\n",
       "      <td>0.119785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043199</td>\n",
       "      <td>-0.012608</td>\n",
       "      <td>0.054249</td>\n",
       "      <td>0.057894</td>\n",
       "      <td>0.015946</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>-0.011760</td>\n",
       "      <td>0.026246</td>\n",
       "      <td>-0.042728</td>\n",
       "      <td>0.018454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>breaking republican member pennsylvania house ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.075425</td>\n",
       "      <td>0.030888</td>\n",
       "      <td>0.045796</td>\n",
       "      <td>0.009012</td>\n",
       "      <td>-0.080354</td>\n",
       "      <td>-0.082336</td>\n",
       "      <td>-0.008419</td>\n",
       "      <td>0.119384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043581</td>\n",
       "      <td>-0.012802</td>\n",
       "      <td>0.054780</td>\n",
       "      <td>0.058419</td>\n",
       "      <td>0.016179</td>\n",
       "      <td>0.008328</td>\n",
       "      <td>-0.012356</td>\n",
       "      <td>0.026490</td>\n",
       "      <td>-0.042990</td>\n",
       "      <td>0.018683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>today joined 8 colleague introducing walk with...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.076657</td>\n",
       "      <td>0.030596</td>\n",
       "      <td>0.045268</td>\n",
       "      <td>0.010127</td>\n",
       "      <td>-0.080485</td>\n",
       "      <td>-0.082908</td>\n",
       "      <td>-0.008490</td>\n",
       "      <td>0.120148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044267</td>\n",
       "      <td>-0.013269</td>\n",
       "      <td>0.055046</td>\n",
       "      <td>0.058660</td>\n",
       "      <td>0.016116</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>-0.012054</td>\n",
       "      <td>0.026910</td>\n",
       "      <td>-0.043462</td>\n",
       "      <td>0.018230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>introducing legislation ban use tear gas distr...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.076289</td>\n",
       "      <td>0.030963</td>\n",
       "      <td>0.045718</td>\n",
       "      <td>0.009371</td>\n",
       "      <td>-0.080902</td>\n",
       "      <td>-0.083031</td>\n",
       "      <td>-0.008512</td>\n",
       "      <td>0.120549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044090</td>\n",
       "      <td>-0.012860</td>\n",
       "      <td>0.055055</td>\n",
       "      <td>0.058658</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>0.008125</td>\n",
       "      <td>-0.011971</td>\n",
       "      <td>0.026665</td>\n",
       "      <td>-0.043584</td>\n",
       "      <td>0.018848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449 rows × 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweets sentiment  feature_0   \n",
       "0    ngoc pham 83 yr old vietnamese man one 2 asian...  negative   0.076744  \\\n",
       "1    making following announcement formal apology t...  positive   0.076372   \n",
       "2    aggressive field operation california sander a...  positive   0.076763   \n",
       "3    thank supervisor aaronpeskin endorsing senator...  positive   0.075840   \n",
       "4    tbt sharing meal friend narendramodi wishing l...  positive   0.076430   \n",
       "..                                                 ...       ...        ...   \n",
       "444  hey united board one flight sure hope given fl...   neutral   0.077678   \n",
       "445  official welcome mikepencevp let first officia...  positive   0.075786   \n",
       "446  breaking republican member pennsylvania house ...  negative   0.075425   \n",
       "447  today joined 8 colleague introducing walk with...  negative   0.076657   \n",
       "448  introducing legislation ban use tear gas distr...  negative   0.076289   \n",
       "\n",
       "     feature_1  feature_2  feature_3  feature_4  feature_5  feature_6   \n",
       "0     0.030947   0.046186   0.010292  -0.081016  -0.083532  -0.008621  \\\n",
       "1     0.030456   0.045402   0.010186  -0.080341  -0.082916  -0.008668   \n",
       "2     0.030808   0.045769   0.009861  -0.080629  -0.082838  -0.008533   \n",
       "3     0.030218   0.045851   0.009287  -0.080463  -0.081054  -0.007640   \n",
       "4     0.030484   0.045450   0.010247  -0.080818  -0.082362  -0.008108   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "444   0.031790   0.046427   0.010435  -0.081350  -0.083798  -0.008335   \n",
       "445   0.030676   0.045305   0.009415  -0.080681  -0.082691  -0.008398   \n",
       "446   0.030888   0.045796   0.009012  -0.080354  -0.082336  -0.008419   \n",
       "447   0.030596   0.045268   0.010127  -0.080485  -0.082908  -0.008490   \n",
       "448   0.030963   0.045718   0.009371  -0.080902  -0.083031  -0.008512   \n",
       "\n",
       "     feature_7  ...  feature_490  feature_491  feature_492  feature_493   \n",
       "0     0.120812  ...     0.043578    -0.012962     0.054955     0.058670  \\\n",
       "1     0.119791  ...     0.043976    -0.013322     0.054382     0.058117   \n",
       "2     0.120222  ...     0.044042    -0.012846     0.055301     0.058937   \n",
       "3     0.117877  ...     0.046128    -0.014048     0.057207     0.060641   \n",
       "4     0.119774  ...     0.043510    -0.013013     0.054999     0.058280   \n",
       "..         ...  ...          ...          ...          ...          ...   \n",
       "444   0.122458  ...     0.043204    -0.012310     0.055704     0.058883   \n",
       "445   0.119785  ...     0.043199    -0.012608     0.054249     0.057894   \n",
       "446   0.119384  ...     0.043581    -0.012802     0.054780     0.058419   \n",
       "447   0.120148  ...     0.044267    -0.013269     0.055046     0.058660   \n",
       "448   0.120549  ...     0.044090    -0.012860     0.055055     0.058658   \n",
       "\n",
       "     feature_494  feature_495  feature_496  feature_497  feature_498   \n",
       "0       0.016228     0.008017    -0.011431     0.026834    -0.042793  \\\n",
       "1       0.016160     0.008301    -0.011921     0.026654    -0.042991   \n",
       "2       0.016421     0.008672    -0.011221     0.026781    -0.043898   \n",
       "3       0.015682     0.010607    -0.013825     0.027092    -0.045812   \n",
       "4       0.015952     0.008525    -0.011212     0.026109    -0.043871   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "444     0.016722     0.007232    -0.010740     0.026866    -0.042810   \n",
       "445     0.015946     0.008372    -0.011760     0.026246    -0.042728   \n",
       "446     0.016179     0.008328    -0.012356     0.026490    -0.042990   \n",
       "447     0.016116     0.008503    -0.012054     0.026910    -0.043462   \n",
       "448     0.016414     0.008125    -0.011971     0.026665    -0.043584   \n",
       "\n",
       "     feature_499  \n",
       "0       0.018279  \n",
       "1       0.017968  \n",
       "2       0.018478  \n",
       "3       0.019946  \n",
       "4       0.018485  \n",
       "..           ...  \n",
       "444     0.018456  \n",
       "445     0.018454  \n",
       "446     0.018683  \n",
       "447     0.018230  \n",
       "448     0.018848  \n",
       "\n",
       "[449 rows x 502 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train the Word2Vec model\n",
    "\n",
    "model = Word2Vec(sentences=df['tweets'], vector_size=500, window=5, sg=1, min_count=1)\n",
    "\n",
    "# Create a list to store tweet embeddings\n",
    "\n",
    "tweet_embeddings = []\n",
    "\n",
    "# Calculate the average embedding for each token in each tweet\n",
    "\n",
    "for tokens in df['tweets']:\n",
    "    embeddings = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    if embeddings:\n",
    "        tweet_embedding = sum(embeddings) / len(embeddings)\n",
    "    else:\n",
    "        tweet_embedding = [0] * model.vector_size  # Assigning zero vector for out-of-vocabulary words\n",
    "        \n",
    "    tweet_embeddings.append(tweet_embedding)\n",
    "\n",
    "# Convert the list of embeddings to a DataFrame\n",
    "tweet_embeddings_df = pd.DataFrame(tweet_embeddings, columns=[f'feature_{i}' for i in range(model.vector_size)])\n",
    "\n",
    "# Concatenate the original DataFrame with the tweet embeddings DataFrame\n",
    "word2vec_df = pd.concat([df, tweet_embeddings_df], axis=1)\n",
    "\n",
    "# Remove the sentence_embeddings column\n",
    "word2vec_df = word2vec_df.drop(columns=['sentence_embeddings'])\n",
    "\n",
    "# Display the modified DataFrame\n",
    "word2vec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Embeddings Creation using BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_758</th>\n",
       "      <th>feature_759</th>\n",
       "      <th>feature_760</th>\n",
       "      <th>feature_761</th>\n",
       "      <th>feature_762</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ngoc pham 83 yr old vietnamese man one 2 asian...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.151141</td>\n",
       "      <td>0.182959</td>\n",
       "      <td>0.402899</td>\n",
       "      <td>-0.305875</td>\n",
       "      <td>0.315593</td>\n",
       "      <td>0.111646</td>\n",
       "      <td>0.240216</td>\n",
       "      <td>0.425379</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032087</td>\n",
       "      <td>-0.201522</td>\n",
       "      <td>0.287459</td>\n",
       "      <td>-0.229566</td>\n",
       "      <td>0.076743</td>\n",
       "      <td>0.075291</td>\n",
       "      <td>-0.147191</td>\n",
       "      <td>-0.154748</td>\n",
       "      <td>-0.033774</td>\n",
       "      <td>0.058169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>making following announcement formal apology t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.019772</td>\n",
       "      <td>-0.119633</td>\n",
       "      <td>0.667289</td>\n",
       "      <td>-0.449308</td>\n",
       "      <td>0.104519</td>\n",
       "      <td>-0.161934</td>\n",
       "      <td>0.494147</td>\n",
       "      <td>0.455437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321194</td>\n",
       "      <td>-0.011513</td>\n",
       "      <td>0.353712</td>\n",
       "      <td>-0.126999</td>\n",
       "      <td>0.142262</td>\n",
       "      <td>-0.122046</td>\n",
       "      <td>-0.075087</td>\n",
       "      <td>-0.075467</td>\n",
       "      <td>0.016284</td>\n",
       "      <td>0.045438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aggressive field operation california sander a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.131454</td>\n",
       "      <td>-0.080869</td>\n",
       "      <td>0.196176</td>\n",
       "      <td>-0.031682</td>\n",
       "      <td>0.272633</td>\n",
       "      <td>-0.090877</td>\n",
       "      <td>-0.003943</td>\n",
       "      <td>0.249903</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086666</td>\n",
       "      <td>-0.123839</td>\n",
       "      <td>-0.198244</td>\n",
       "      <td>-0.371338</td>\n",
       "      <td>0.314498</td>\n",
       "      <td>0.145142</td>\n",
       "      <td>-0.303607</td>\n",
       "      <td>-0.183542</td>\n",
       "      <td>-0.109382</td>\n",
       "      <td>-0.375275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank supervisor aaronpeskin endorsing senator...</td>\n",
       "      <td>positive</td>\n",
       "      <td>-0.179532</td>\n",
       "      <td>-0.021509</td>\n",
       "      <td>0.155459</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>-0.155799</td>\n",
       "      <td>-0.217436</td>\n",
       "      <td>-0.008971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092380</td>\n",
       "      <td>-0.059386</td>\n",
       "      <td>0.015992</td>\n",
       "      <td>0.098745</td>\n",
       "      <td>0.338740</td>\n",
       "      <td>0.007371</td>\n",
       "      <td>-0.223407</td>\n",
       "      <td>0.085241</td>\n",
       "      <td>-0.032808</td>\n",
       "      <td>0.119657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tbt sharing meal friend narendramodi wishing l...</td>\n",
       "      <td>positive</td>\n",
       "      <td>-0.295412</td>\n",
       "      <td>-0.332681</td>\n",
       "      <td>0.301100</td>\n",
       "      <td>-0.227789</td>\n",
       "      <td>0.419574</td>\n",
       "      <td>-0.102872</td>\n",
       "      <td>0.474347</td>\n",
       "      <td>0.239754</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031311</td>\n",
       "      <td>0.160330</td>\n",
       "      <td>0.354961</td>\n",
       "      <td>-0.036209</td>\n",
       "      <td>-0.098433</td>\n",
       "      <td>0.072526</td>\n",
       "      <td>0.065840</td>\n",
       "      <td>-0.350750</td>\n",
       "      <td>0.079152</td>\n",
       "      <td>-0.117877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>hey united board one flight sure hope given fl...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>-0.260867</td>\n",
       "      <td>0.497267</td>\n",
       "      <td>-0.093050</td>\n",
       "      <td>0.434605</td>\n",
       "      <td>-0.042255</td>\n",
       "      <td>0.389752</td>\n",
       "      <td>0.375278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004017</td>\n",
       "      <td>-0.408482</td>\n",
       "      <td>0.171101</td>\n",
       "      <td>-0.064507</td>\n",
       "      <td>-0.007335</td>\n",
       "      <td>0.129339</td>\n",
       "      <td>-0.141256</td>\n",
       "      <td>-0.311000</td>\n",
       "      <td>-0.030480</td>\n",
       "      <td>-0.302863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>official welcome mikepencevp let first officia...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.375342</td>\n",
       "      <td>0.046244</td>\n",
       "      <td>0.656258</td>\n",
       "      <td>-0.028738</td>\n",
       "      <td>0.071722</td>\n",
       "      <td>-0.131102</td>\n",
       "      <td>0.444305</td>\n",
       "      <td>0.248182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204532</td>\n",
       "      <td>-0.196501</td>\n",
       "      <td>0.063307</td>\n",
       "      <td>-0.169355</td>\n",
       "      <td>0.228348</td>\n",
       "      <td>0.204946</td>\n",
       "      <td>-0.358443</td>\n",
       "      <td>-0.099012</td>\n",
       "      <td>-0.088422</td>\n",
       "      <td>-0.123316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>breaking republican member pennsylvania house ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.049125</td>\n",
       "      <td>-0.283249</td>\n",
       "      <td>-0.012151</td>\n",
       "      <td>0.157952</td>\n",
       "      <td>0.084787</td>\n",
       "      <td>-0.151598</td>\n",
       "      <td>0.047349</td>\n",
       "      <td>-0.334749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086166</td>\n",
       "      <td>0.273918</td>\n",
       "      <td>-0.085442</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.475979</td>\n",
       "      <td>-0.040850</td>\n",
       "      <td>-0.036262</td>\n",
       "      <td>-0.077140</td>\n",
       "      <td>-0.142727</td>\n",
       "      <td>0.006540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>today joined 8 colleague introducing walk with...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.151372</td>\n",
       "      <td>-0.096814</td>\n",
       "      <td>0.387434</td>\n",
       "      <td>-0.147987</td>\n",
       "      <td>0.238097</td>\n",
       "      <td>-0.200005</td>\n",
       "      <td>0.261640</td>\n",
       "      <td>0.234046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222576</td>\n",
       "      <td>0.150740</td>\n",
       "      <td>0.143877</td>\n",
       "      <td>-0.265047</td>\n",
       "      <td>0.493757</td>\n",
       "      <td>-0.082712</td>\n",
       "      <td>-0.207422</td>\n",
       "      <td>-0.457475</td>\n",
       "      <td>-0.221328</td>\n",
       "      <td>-0.357937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>introducing legislation ban use tear gas distr...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.309776</td>\n",
       "      <td>-0.021357</td>\n",
       "      <td>-0.123608</td>\n",
       "      <td>-0.259325</td>\n",
       "      <td>-0.038622</td>\n",
       "      <td>-0.198144</td>\n",
       "      <td>0.340735</td>\n",
       "      <td>-0.006078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168060</td>\n",
       "      <td>0.057625</td>\n",
       "      <td>0.129071</td>\n",
       "      <td>-0.086706</td>\n",
       "      <td>0.475074</td>\n",
       "      <td>-0.186304</td>\n",
       "      <td>-0.009041</td>\n",
       "      <td>-0.338859</td>\n",
       "      <td>-0.281377</td>\n",
       "      <td>-0.056604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449 rows × 770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweets sentiment  feature_0   \n",
       "0    ngoc pham 83 yr old vietnamese man one 2 asian...  negative   0.151141  \\\n",
       "1    making following announcement formal apology t...  positive   0.019772   \n",
       "2    aggressive field operation california sander a...  positive   0.131454   \n",
       "3    thank supervisor aaronpeskin endorsing senator...  positive  -0.179532   \n",
       "4    tbt sharing meal friend narendramodi wishing l...  positive  -0.295412   \n",
       "..                                                 ...       ...        ...   \n",
       "444  hey united board one flight sure hope given fl...   neutral   0.160700   \n",
       "445  official welcome mikepencevp let first officia...  positive   0.375342   \n",
       "446  breaking republican member pennsylvania house ...  negative   0.049125   \n",
       "447  today joined 8 colleague introducing walk with...  negative   0.151372   \n",
       "448  introducing legislation ban use tear gas distr...  negative   0.309776   \n",
       "\n",
       "     feature_1  feature_2  feature_3  feature_4  feature_5  feature_6   \n",
       "0     0.182959   0.402899  -0.305875   0.315593   0.111646   0.240216  \\\n",
       "1    -0.119633   0.667289  -0.449308   0.104519  -0.161934   0.494147   \n",
       "2    -0.080869   0.196176  -0.031682   0.272633  -0.090877  -0.003943   \n",
       "3    -0.021509   0.155459   0.038721   0.035100  -0.155799  -0.217436   \n",
       "4    -0.332681   0.301100  -0.227789   0.419574  -0.102872   0.474347   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "444  -0.260867   0.497267  -0.093050   0.434605  -0.042255   0.389752   \n",
       "445   0.046244   0.656258  -0.028738   0.071722  -0.131102   0.444305   \n",
       "446  -0.283249  -0.012151   0.157952   0.084787  -0.151598   0.047349   \n",
       "447  -0.096814   0.387434  -0.147987   0.238097  -0.200005   0.261640   \n",
       "448  -0.021357  -0.123608  -0.259325  -0.038622  -0.198144   0.340735   \n",
       "\n",
       "     feature_7  ...  feature_758  feature_759  feature_760  feature_761   \n",
       "0     0.425379  ...    -0.032087    -0.201522     0.287459    -0.229566  \\\n",
       "1     0.455437  ...     0.321194    -0.011513     0.353712    -0.126999   \n",
       "2     0.249903  ...    -0.086666    -0.123839    -0.198244    -0.371338   \n",
       "3    -0.008971  ...    -0.092380    -0.059386     0.015992     0.098745   \n",
       "4     0.239754  ...    -0.031311     0.160330     0.354961    -0.036209   \n",
       "..         ...  ...          ...          ...          ...          ...   \n",
       "444   0.375278  ...    -0.004017    -0.408482     0.171101    -0.064507   \n",
       "445   0.248182  ...     0.204532    -0.196501     0.063307    -0.169355   \n",
       "446  -0.334749  ...     0.086166     0.273918    -0.085442     0.097800   \n",
       "447   0.234046  ...     0.222576     0.150740     0.143877    -0.265047   \n",
       "448  -0.006078  ...     0.168060     0.057625     0.129071    -0.086706   \n",
       "\n",
       "     feature_762  feature_763  feature_764  feature_765  feature_766   \n",
       "0       0.076743     0.075291    -0.147191    -0.154748    -0.033774  \\\n",
       "1       0.142262    -0.122046    -0.075087    -0.075467     0.016284   \n",
       "2       0.314498     0.145142    -0.303607    -0.183542    -0.109382   \n",
       "3       0.338740     0.007371    -0.223407     0.085241    -0.032808   \n",
       "4      -0.098433     0.072526     0.065840    -0.350750     0.079152   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "444    -0.007335     0.129339    -0.141256    -0.311000    -0.030480   \n",
       "445     0.228348     0.204946    -0.358443    -0.099012    -0.088422   \n",
       "446     0.475979    -0.040850    -0.036262    -0.077140    -0.142727   \n",
       "447     0.493757    -0.082712    -0.207422    -0.457475    -0.221328   \n",
       "448     0.475074    -0.186304    -0.009041    -0.338859    -0.281377   \n",
       "\n",
       "     feature_767  \n",
       "0       0.058169  \n",
       "1       0.045438  \n",
       "2      -0.375275  \n",
       "3       0.119657  \n",
       "4      -0.117877  \n",
       "..           ...  \n",
       "444    -0.302863  \n",
       "445    -0.123316  \n",
       "446     0.006540  \n",
       "447    -0.357937  \n",
       "448    -0.056604  \n",
       "\n",
       "[449 rows x 770 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Initialize lists to store sentence embeddings\n",
    "sentence_embeddings_list = []\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    sentence = row['tweets']\n",
    "    \n",
    "    # Tokenize the sentence and convert to token IDs\n",
    "    tokens = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    token_ids = torch.tensor(tokens).unsqueeze(0)\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = token_ids != 0\n",
    "    \n",
    "    # Run a forward pass through BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(token_ids, attention_mask)\n",
    "    encoded_layer = outputs.last_hidden_state\n",
    "    \n",
    "    # Calculate sentence embedding (mean of all word embeddings)\n",
    "    sentence_embedding = encoded_layer.mean(dim=1)\n",
    "    sentence_embeddings_list.append(sentence_embedding[0].tolist())\n",
    "\n",
    "# Convert sentence embeddings to a NumPy array\n",
    "sentence_embeddings_array = torch.tensor(sentence_embeddings_list).numpy()\n",
    "\n",
    "# Create a new DataFrame with original tweets, sentiment, and sentence embedding features\n",
    "embedding_df = pd.concat([\n",
    "    df[['tweets', 'sentiment']],\n",
    "    pd.DataFrame(sentence_embeddings_array, columns=[f\"feature_{i}\" for i in range(sentence_embeddings_array.shape[1])])\n",
    "], axis=1)\n",
    "\n",
    "# Display the embedding DataFrame\n",
    "embedding_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating the Random Forest Classification Model on tfidf word embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target\n",
    "\n",
    "X = tfidf_df.iloc[:, 2:]\n",
    "y = tfidf_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=500)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=500)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an SVM model\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=500)\n",
    "model_tfidf = rfc.fit(X_train, y_train)\n",
    "model_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'neutral', 'negative', 'positive', 'negative',\n",
       "       'negative', 'negative', 'negative', 'neutral', 'neutral',\n",
       "       'neutral', 'positive', 'neutral', 'positive', 'neutral',\n",
       "       'negative', 'positive', 'neutral', 'positive', 'neutral',\n",
       "       'negative', 'neutral', 'positive', 'negative', 'negative',\n",
       "       'negative', 'negative', 'neutral', 'neutral', 'negative',\n",
       "       'neutral', 'negative', 'negative', 'neutral', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'negative', 'negative',\n",
       "       'negative', 'neutral', 'neutral', 'negative', 'negative',\n",
       "       'neutral', 'neutral', 'negative', 'positive', 'negative',\n",
       "       'negative', 'neutral', 'positive', 'neutral', 'neutral', 'neutral',\n",
       "       'negative', 'negative', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'positive', 'negative', 'negative', 'negative',\n",
       "       'neutral', 'negative', 'positive', 'negative', 'neutral',\n",
       "       'neutral', 'positive', 'positive', 'negative', 'positive',\n",
       "       'negative', 'negative', 'positive', 'positive', 'neutral',\n",
       "       'neutral', 'neutral', 'neutral', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_rfc_tfidf = rfc.predict(X_test)\n",
    "y_pred_rfc_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n",
      "Confusion Matrix:\n",
      "[[28  5  1]\n",
      " [ 9 22  1]\n",
      " [ 4  3 17]]\n",
      "Precision: 0.76\n",
      "Recall: 0.74\n",
      "F1-Score: 0.75\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.82      0.75        34\n",
      "     neutral       0.73      0.69      0.71        32\n",
      "    positive       0.89      0.71      0.79        24\n",
      "\n",
      "    accuracy                           0.74        90\n",
      "   macro avg       0.77      0.74      0.75        90\n",
      "weighted avg       0.76      0.74      0.75        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, precision_recall_curve, auc, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_rfc_tfidf)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_rfc_tfidf)\n",
    "precision = precision_score(y_test, y_pred_rfc_tfidf, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_rfc_tfidf, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_rfc_tfidf, average='weighted')\n",
    "class_report = classification_report(y_test, y_pred_rfc_tfidf, zero_division='warn')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating the Support Vector Machines Classification Model on tfidf word embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate the SVM model with a linear kernel\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "# Fit the SVM model to the training data\n",
    "svm_model_tfidf = svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'neutral', 'negative', 'positive', 'negative',\n",
       "       'negative', 'negative', 'negative', 'neutral', 'neutral',\n",
       "       'neutral', 'positive', 'negative', 'positive', 'negative',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'negative', 'positive', 'positive', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'neutral', 'negative',\n",
       "       'negative', 'negative', 'negative', 'neutral', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'negative', 'negative',\n",
       "       'negative', 'neutral', 'neutral', 'negative', 'negative',\n",
       "       'neutral', 'neutral', 'negative', 'positive', 'negative',\n",
       "       'negative', 'neutral', 'positive', 'negative', 'neutral',\n",
       "       'negative', 'negative', 'negative', 'neutral', 'negative',\n",
       "       'negative', 'negative', 'positive', 'negative', 'positive',\n",
       "       'negative', 'neutral', 'neutral', 'positive', 'negative',\n",
       "       'neutral', 'neutral', 'positive', 'positive', 'negative',\n",
       "       'positive', 'negative', 'neutral', 'positive', 'negative',\n",
       "       'neutral', 'neutral', 'negative', 'neutral', 'positive'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_svm_tfidf = svm_model_tfidf.predict(X_test)\n",
    "y_pred_svm_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18     negative\n",
       "30     positive\n",
       "325    negative\n",
       "281    positive\n",
       "442    negative\n",
       "         ...   \n",
       "288     neutral\n",
       "354     neutral\n",
       "378    negative\n",
       "343     neutral\n",
       "427    positive\n",
       "Name: sentiment, Length: 90, dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Confusion Matrix:\n",
      "[[30  3  1]\n",
      " [14 16  2]\n",
      " [ 3  3 18]]\n",
      "Precision: 0.73\n",
      "Recall: 0.71\n",
      "F1-Score: 0.70\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.88      0.74        34\n",
      "     neutral       0.73      0.50      0.59        32\n",
      "    positive       0.86      0.75      0.80        24\n",
      "\n",
      "    accuracy                           0.71        90\n",
      "   macro avg       0.74      0.71      0.71        90\n",
      "weighted avg       0.73      0.71      0.70        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, precision_recall_curve, auc, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_svm_tfidf)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_svm_tfidf)\n",
    "precision = precision_score(y_test, y_pred_svm_tfidf, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_svm_tfidf, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_svm_tfidf, average='weighted')\n",
    "class_report = classification_report(y_test, y_pred_svm_tfidf, zero_division='warn')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating the Random Forest Classification Model on Word2Vec word embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature matrix and Target vector\n",
    "\n",
    "X = word2vec_df.iloc[:, 2:]\n",
    "y = word2vec_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset to train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_model = RandomForestClassifier(n_estimators=500)\n",
    "rfc_model_word2vec = rfc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'negative', 'negative', 'positive', 'negative',\n",
       "       'negative', 'positive', 'negative', 'neutral', 'neutral',\n",
       "       'neutral', 'positive', 'neutral', 'positive', 'neutral',\n",
       "       'negative', 'positive', 'neutral', 'positive', 'negative',\n",
       "       'neutral', 'positive', 'positive', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'neutral', 'neutral',\n",
       "       'neutral', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'negative',\n",
       "       'negative', 'neutral', 'positive', 'negative', 'negative',\n",
       "       'negative', 'positive', 'negative', 'positive', 'negative',\n",
       "       'negative', 'neutral', 'positive', 'neutral', 'neutral', 'neutral',\n",
       "       'negative', 'positive', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'positive', 'negative',\n",
       "       'neutral', 'neutral', 'positive', 'negative', 'neutral',\n",
       "       'negative', 'positive', 'positive', 'neutral', 'negative',\n",
       "       'positive', 'neutral', 'positive', 'positive', 'neutral',\n",
       "       'negative', 'positive', 'neutral', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_rfc_word2vec = rfc_model_word2vec.predict(X_test)\n",
    "y_pred_rfc_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18     negative\n",
       "30     positive\n",
       "325    negative\n",
       "281    positive\n",
       "442    negative\n",
       "         ...   \n",
       "288     neutral\n",
       "354     neutral\n",
       "378    negative\n",
       "343     neutral\n",
       "427    positive\n",
       "Name: sentiment, Length: 90, dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Confusion Matrix:\n",
      "[[26  4  4]\n",
      " [ 8 19  5]\n",
      " [ 7  0 17]]\n",
      "Precision: 0.71\n",
      "Recall: 0.69\n",
      "F1-Score: 0.69\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.76      0.69        34\n",
      "     neutral       0.83      0.59      0.69        32\n",
      "    positive       0.65      0.71      0.68        24\n",
      "\n",
      "    accuracy                           0.69        90\n",
      "   macro avg       0.70      0.69      0.69        90\n",
      "weighted avg       0.71      0.69      0.69        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, precision_recall_curve, auc, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_rfc_word2vec)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_rfc_word2vec)\n",
    "precision = precision_score(y_test, y_pred_rfc_word2vec, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_rfc_word2vec, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_rfc_word2vec, average='weighted')\n",
    "class_report = classification_report(y_test, y_pred_rfc_word2vec, zero_division='warn')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating the Support Vector Machines Classification Model on Word2Vec word embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = word2vec_df.iloc[:, 2:]\n",
    "y = word2vec_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate the SVM model with a linear kernel\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "# Fit the SVM model to the training data\n",
    "svm_model_word2vec = svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "negative    140\n",
       "neutral     115\n",
       "positive    104\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'negative'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_svm_word2vec = svm_model_word2vec.predict(X_test)\n",
    "y_pred_svm_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18     negative\n",
       "30     positive\n",
       "325    negative\n",
       "281    positive\n",
       "442    negative\n",
       "         ...   \n",
       "288     neutral\n",
       "354     neutral\n",
       "378    negative\n",
       "343     neutral\n",
       "427    positive\n",
       "Name: sentiment, Length: 90, dtype: object"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.38\n",
      "Confusion Matrix:\n",
      "[[34  0  0]\n",
      " [32  0  0]\n",
      " [24  0  0]]\n",
      "Precision: 0.14\n",
      "Recall: 0.38\n",
      "F1-Score: 0.21\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      1.00      0.55        34\n",
      "     neutral       0.00      0.00      0.00        32\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.38        90\n",
      "   macro avg       0.13      0.33      0.18        90\n",
      "weighted avg       0.14      0.38      0.21        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dwara\\OneDrive\\Desktop\\Ryan\\CSCN8010\\venv\\CSCN8010_classic_ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dwara\\OneDrive\\Desktop\\Ryan\\CSCN8010\\venv\\CSCN8010_classic_ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dwara\\OneDrive\\Desktop\\Ryan\\CSCN8010\\venv\\CSCN8010_classic_ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dwara\\OneDrive\\Desktop\\Ryan\\CSCN8010\\venv\\CSCN8010_classic_ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, precision_recall_curve, auc, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_svm_word2vec)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_svm_word2vec)\n",
    "precision = precision_score(y_test, y_pred_svm_word2vec, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_svm_word2vec, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_svm_word2vec, average='weighted')\n",
    "class_report = classification_report(y_test, y_pred_svm_word2vec)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating the Random Forest Classification Model on BERT word embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = embedding_df.iloc[:,2:]\n",
    "y = embedding_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_model = RandomForestClassifier(n_estimators=500)\n",
    "rfc_model_BERT = rfc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'negative', 'positive', 'negative',\n",
       "       'positive', 'negative', 'negative', 'negative', 'neutral',\n",
       "       'neutral', 'positive', 'negative', 'positive', 'neutral',\n",
       "       'negative', 'positive', 'neutral', 'positive', 'negative',\n",
       "       'negative', 'negative', 'positive', 'negative', 'negative',\n",
       "       'positive', 'negative', 'neutral', 'neutral', 'negative',\n",
       "       'negative', 'negative', 'negative', 'neutral', 'negative',\n",
       "       'negative', 'negative', 'negative', 'negative', 'positive',\n",
       "       'positive', 'neutral', 'positive', 'negative', 'negative',\n",
       "       'negative', 'neutral', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'positive', 'negative',\n",
       "       'negative', 'neutral', 'positive', 'neutral', 'neutral', 'neutral',\n",
       "       'negative', 'neutral', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'negative', 'negative', 'positive', 'positive',\n",
       "       'neutral', 'negative', 'positive', 'negative', 'neutral',\n",
       "       'neutral', 'negative', 'positive', 'negative', 'positive',\n",
       "       'positive', 'negative', 'positive', 'positive', 'neutral',\n",
       "       'negative', 'negative', 'neutral', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_rfc_BERT = rfc_model_BERT.predict(X_test)\n",
    "y_pred_rfc_BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n",
      "Confusion Matrix:\n",
      "[[34  0  0]\n",
      " [11 19  2]\n",
      " [ 1  2 21]]\n",
      "Precision: 0.84\n",
      "Recall: 0.82\n",
      "F1-Score: 0.81\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        34\n",
      "     neutral       0.90      0.59      0.72        32\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.82        90\n",
      "   macro avg       0.85      0.82      0.82        90\n",
      "weighted avg       0.84      0.82      0.81        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, precision_recall_curve, auc, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_rfc_BERT)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_rfc_BERT)\n",
    "precision = precision_score(y_test, y_pred_rfc_BERT, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_rfc_BERT, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_rfc_BERT, average='weighted')\n",
    "class_report = classification_report(y_test, y_pred_rfc_BERT, zero_division='warn')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating the SVM Classification Model on BERT word embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate the SVM model with a linear kernel\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "# Fit the SVM model to the training data\n",
    "svm_model_BERT = svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'negative', 'positive', 'negative',\n",
       "       'positive', 'negative', 'negative', 'negative', 'neutral',\n",
       "       'neutral', 'positive', 'neutral', 'positive', 'neutral',\n",
       "       'negative', 'positive', 'positive', 'positive', 'negative',\n",
       "       'negative', 'neutral', 'positive', 'negative', 'negative',\n",
       "       'neutral', 'negative', 'neutral', 'neutral', 'negative', 'neutral',\n",
       "       'negative', 'negative', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'neutral', 'neutral', 'positive', 'positive',\n",
       "       'positive', 'positive', 'negative', 'negative', 'negative',\n",
       "       'neutral', 'neutral', 'negative', 'negative', 'negative',\n",
       "       'negative', 'negative', 'positive', 'negative', 'negative',\n",
       "       'neutral', 'positive', 'neutral', 'neutral', 'negative',\n",
       "       'negative', 'negative', 'neutral', 'negative', 'negative',\n",
       "       'negative', 'neutral', 'negative', 'positive', 'positive',\n",
       "       'neutral', 'neutral', 'positive', 'negative', 'neutral', 'neutral',\n",
       "       'neutral', 'positive', 'negative', 'positive', 'neutral',\n",
       "       'negative', 'positive', 'positive', 'neutral', 'neutral',\n",
       "       'negative', 'neutral', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_svm_BERT = svm_model_BERT.predict(X_test)\n",
    "y_pred_svm_BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "Confusion Matrix:\n",
      "[[30  4  0]\n",
      " [ 9 21  2]\n",
      " [ 1  3 20]]\n",
      "Precision: 0.79\n",
      "Recall: 0.79\n",
      "F1-Score: 0.79\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.88      0.81        34\n",
      "     neutral       0.75      0.66      0.70        32\n",
      "    positive       0.91      0.83      0.87        24\n",
      "\n",
      "    accuracy                           0.79        90\n",
      "   macro avg       0.80      0.79      0.79        90\n",
      "weighted avg       0.79      0.79      0.79        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, precision_recall_curve, auc, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_svm_BERT)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_svm_BERT)\n",
    "precision = precision_score(y_test, y_pred_svm_BERT, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_svm_BERT, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_svm_BERT, average='weighted')\n",
    "class_report = classification_report(y_test, y_pred_svm_BERT, zero_division='warn')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
