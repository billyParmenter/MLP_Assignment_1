{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.common.by import By\n",
    "import string\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import contractions\n",
    "from autocorrect import Speller\n",
    "\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from slangs import slangs_dict\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "fix_spelling = pipeline(\"text2text-generation\",model=\"grantslewis/spelling-correction-english-base-location-unique-2-2-proportional\")\n",
    "\n",
    "def get_user_tweets(user):\n",
    "\n",
    "    url = f'https://twitter.com/{user}'\n",
    "\n",
    "    # Define the options for the Chrome webdriver\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')\n",
    "    options.add_argument(\"--enable-javascript\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Load the page\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for tweets to populate the page\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(expected_conditions.presence_of_element_located(\n",
    "            (By.CSS_SELECTOR, '[data-testid=\"tweet\"]')))\n",
    "    except WebDriverException:\n",
    "        return\n",
    "\n",
    "    # Extract the HTML content\n",
    "    html_content = driver.page_source\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    posts = soup.find_all(attrs={\"data-testid\": \"tweetText\"})\n",
    "\n",
    "    # Extract the text content of each post\n",
    "    post_content = [post.text for post in posts]\n",
    "\n",
    "    # Save the data in a CSV file\n",
    "    data = pd.DataFrame(post_content)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return(data)\n",
    "\n",
    "def get_tweets():\n",
    "    df = pd.read_csv('./data/users.csv', delimiter=',')\n",
    "\n",
    "    tweets = pd.DataFrame()\n",
    "\n",
    "    for i in range(0, len(df)):\n",
    "        if i % 10 == 0:\n",
    "            print(i,'/',len(df)) \n",
    "\n",
    "        tweets = get_user_tweets(df['Twitter_username'][i])\n",
    "\n",
    "        try:\n",
    "            tweets.to_csv('./data/twitter_posts.csv', mode='a',index=False, header=False)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "def clean_df(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna()\n",
    "    df.apply(preprocess, axis=1)\n",
    "    df.apply(clean_text, axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocess(text_to_clean):\n",
    "    try:\n",
    "        step = 'slang'        \n",
    "        text_to_clean['text'] = ' '.join([slangs_dict[word.lower()] if word.lower() in slangs_dict.keys() else word for word in text_to_clean['text'].split()])\n",
    "\n",
    "        step = 'contractions'\n",
    "        text_to_clean['text'] = ' '.join([contractions.fix(word) for word in text_to_clean['text'].split()])\n",
    "\n",
    "        step = 'attached'\n",
    "        text_to_clean['text'] = re.sub(r'(?<=[^\\s])(?=[A-Z])', ' ', text_to_clean['text'])\n",
    "\n",
    "        step = 'numbers and words'\n",
    "        text_to_clean['text'] = re.sub(r'(?<=\\D)(?=\\d)|(?<=\\d)(?=\\D)', ' ', text_to_clean['text'])\n",
    "\n",
    "        step = 'regex'\n",
    "        text_to_clean['text'] = re.sub('@[^\\s]+',' ',text_to_clean['text'])\n",
    "        text_to_clean['text'] = re.sub('http[^\\s]+',' ',text_to_clean['text'])\n",
    "        text_to_clean['text'] = re.sub('[^A-Za-z0-9\\' ]+',' ',text_to_clean['text'])\n",
    "\n",
    "        step = 'spaces'\n",
    "        text_to_clean['text'] = re.sub(r'\\s+', ' ', text_to_clean['text']).strip()\n",
    "\n",
    "        step = 'spell'\n",
    "        text_to_clean['text'] =  fix_spelling(text_to_clean['text'],max_length=2048)[0]['generated_text']\n",
    "\n",
    "        step = 'contractions2'\n",
    "        text_to_clean['text'] = ' '.join([contractions.fix(word) for word in text_to_clean['text'].split()])\n",
    "        text_to_clean['text'] = re.sub('[^A-Za-z0-9\\' ]+',' ',text_to_clean['text'])\n",
    "        \n",
    "        step = 'spaces'\n",
    "        text_to_clean['text'] = re.sub(r'\\s+', ' ', text_to_clean['text']).strip()\n",
    "\n",
    "        return text_to_clean\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Failed at {step} with: {text_to_clean}, {e}')\n",
    "\n",
    "\n",
    "def clean_text(text_to_clean):\n",
    "    try:\n",
    "        punctuation_list = list(string.punctuation)\n",
    "\n",
    "        step = 'token'\n",
    "        text_to_clean['text'] = word_tokenize(text_to_clean['text'])\n",
    "\n",
    "        step = 'stop'\n",
    "        text_to_clean['text'] = [word for word in text_to_clean['text'] if word not in stopwords.words('english')]\n",
    "\n",
    "        step = 'punct'\n",
    "        text_to_clean['text'] = [word for word in text_to_clean['text'] if word not in punctuation_list]\n",
    "\n",
    "        step = 'lower'\n",
    "        text_to_clean['text'] = [word.lower() for word in text_to_clean['text']]\n",
    "       \n",
    "        return  text_to_clean['text']\n",
    "    except Exception as e:\n",
    "        print(f'Failed at {step} with: {text_to_clean}, {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngoc Pham, a 83 yr old Vietnamese man, was one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I’m making the following announcement and form...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“The most aggressive field operation in Califo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you Supervisor @AaronPeskin for endorsin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#tbt sharing a meal w/my friend @narendramodi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>Unborn children should be welcomed in life and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>I will not vote for John Boehner.  My statemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>I believe that warriors like Chris Kyle are so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>Millions of Americans stand with #Israel.  RT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>I congratulate @replouiegohmert &amp; @RepTedYoho ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1828 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     Ngoc Pham, a 83 yr old Vietnamese man, was one...\n",
       "1     I’m making the following announcement and form...\n",
       "2     “The most aggressive field operation in Califo...\n",
       "3     Thank you Supervisor @AaronPeskin for endorsin...\n",
       "4     #tbt sharing a meal w/my friend @narendramodi ...\n",
       "...                                                 ...\n",
       "1823  Unborn children should be welcomed in life and...\n",
       "1824  I will not vote for John Boehner.  My statemen...\n",
       "1825  I believe that warriors like Chris Kyle are so...\n",
       "1826  Millions of Americans stand with #Israel.  RT ...\n",
       "1827  I congratulate @replouiegohmert & @RepTedYoho ...\n",
       "\n",
       "[1828 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('./data//twitter_posts.csv', delimiter=',', names=['text'], header=None)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre\n",
      "~~~~ Before preprocess ~~~~\n",
      "Ngoc Pham, a 83 yr old Vietnamese man, was one of 2 Asian seniors attacked by the same perpetrator yesterday. He survived 17 yrs in a Vietnamese concentration camp, only to be assaulted while grocery shopping. His son has asked us all to get his story out.\n",
      "\n",
      "\tslang\n",
      "Ngoc Pham, a 83 year old Vietnamese man, was one of 2 Asian seniors attacked by the same perpetrator yesterday. He survived 17 years in a Vietnamese concentration camp, only to be assaulted while grocery shopping. His son has asked us all to get his story out.\n",
      "\tcontractions\n",
      "Ngoc Pham, a 83 year old Vietnamese man, was one of 2 Asian seniors attacked by the same perpetrator yesterday. He survived 17 years in a Vietnamese concentration camp, only to be assaulted while grocery shopping. His son has asked us all to get his story out.\n",
      "\tattached\n",
      "Ngoc Pham, a 83 year old Vietnamese man, was one of 2 Asian seniors attacked by the same perpetrator yesterday. He survived 17 years in a Vietnamese concentration camp, only to be assaulted while grocery shopping. His son has asked us all to get his story out.\n",
      "\tnumbers and words\n",
      "Ngoc Pham, a  83  year old Vietnamese man, was one of  2  Asian seniors attacked by the same perpetrator yesterday. He survived  17  years in a Vietnamese concentration camp, only to be assaulted while grocery shopping. His son has asked us all to get his story out.\n",
      "\tregex\n",
      "Ngoc Pham  a  83  year old Vietnamese man  was one of  2  Asian seniors attacked by the same perpetrator yesterday  He survived  17  years in a Vietnamese concentration camp  only to be assaulted while grocery shopping  His son has asked us all to get his story out \n",
      "\tspaces\n",
      "Ngoc Pham a 83 year old Vietnamese man was one of 2 Asian seniors attacked by the same perpetrator yesterday He survived 17 years in a Vietnamese concentration camp only to be assaulted while grocery shopping His son has asked us all to get his story out\n",
      "\tspell\n",
      "Ngoc Pham A 83 year old Vietnamese man was one of two Asian seniors attacked by the same perpetrator yesterday. He survived 17 years in a Vietnamese concentration camp only to be assaulted while grocery shopping. His son has asked us all to get his story out.\n",
      "Ngoc Pham A 83 year old Vietnamese man was one of two Asian seniors attacked by the same perpetrator yesterday He survived 17 years in a Vietnamese concentration camp only to be assaulted while grocery shopping His son has asked us all to get his story out\n",
      "\tcontractions2\n",
      "Ngoc Pham A 83 year old Vietnamese man was one of two Asian seniors attacked by the same perpetrator yesterday He survived 17 years in a Vietnamese concentration camp only to be assaulted while grocery shopping His son has asked us all to get his story out\n",
      "\n",
      "~~~~ After ~~~~\n",
      "Ngoc Pham A 83 year old Vietnamese man was one of two Asian seniors attacked by the same perpetrator yesterday He survived 17 years in a Vietnamese concentration camp only to be assaulted while grocery shopping His son has asked us all to get his story out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "claen\n"
     ]
    }
   ],
   "source": [
    "print('pre')\n",
    "text = preprocess(tweets.loc[0])\n",
    "print('\\nclaen')\n",
    "# clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ~30 min\n",
    "# tweets = clean_df(tweets)\n",
    "# tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# API_URL = \"https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "# headers = {\"Authorization\": f\"Bearer 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\"}\n",
    "\n",
    "# def query(payload):\n",
    "# \tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "# \treturn response.json()\n",
    "\t\n",
    "# output = query({\n",
    "# \t\"inputs\": [\"I like you. I love you\",\n",
    "# \t    \"no hate, don't like\"]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(fix_spelling(\"lets do a comparsion\",max_length=2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(fix_spelling(\"Millions of Americans stand with Israel retweet if you do too I Stand With Israel tcot\",max_length=2048))\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
