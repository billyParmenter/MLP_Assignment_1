{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.common.by import By\n",
    "import string\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import contractions\n",
    "from autocorrect import Speller\n",
    "\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from slangs import slangs_dict\n",
    "\n",
    "def get_user_tweets(user):\n",
    "\n",
    "    url = f'https://twitter.com/{user}'\n",
    "\n",
    "    # Define the options for the Chrome webdriver\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')\n",
    "    options.add_argument(\"--enable-javascript\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Load the page\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for tweets to populate the page\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(expected_conditions.presence_of_element_located(\n",
    "            (By.CSS_SELECTOR, '[data-testid=\"tweet\"]')))\n",
    "    except WebDriverException:\n",
    "        return\n",
    "\n",
    "    # Extract the HTML content\n",
    "    html_content = driver.page_source\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    posts = soup.find_all(attrs={\"data-testid\": \"tweetText\"})\n",
    "\n",
    "    # Extract the text content of each post\n",
    "    post_content = [post.text for post in posts]\n",
    "\n",
    "    # Save the data in a CSV file\n",
    "    data = pd.DataFrame(post_content)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return(data)\n",
    "\n",
    "def get_tweets():\n",
    "    df = pd.read_csv('./data/users.csv', delimiter=',')\n",
    "\n",
    "    tweets = pd.DataFrame()\n",
    "\n",
    "    for i in range(0, len(df)):\n",
    "        if i % 10 == 0:\n",
    "            print(i,'/',len(df)) \n",
    "\n",
    "        tweets = get_user_tweets(df['Twitter_username'][i])\n",
    "\n",
    "        try:\n",
    "            tweets.to_csv('./data/twitter_posts.csv', mode='a',index=False, header=False)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "def clean_df(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna()\n",
    "    df.apply(clean_text, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_text(text_to_clean):\n",
    "    try:\n",
    "        punctuation_list = list(string.punctuation)\n",
    "\n",
    "        step = 'contractions'\n",
    "        # print(text_to_clean['text'])\n",
    "        # print('contractions')\n",
    "\n",
    "        text_to_clean['text'] = ' '.join([contractions.fix(word) for word in text_to_clean['text'].split()])\n",
    "\n",
    "        # step = 'slang'\n",
    "        # print(text_to_clean['text'])\n",
    "        # print('slang')\n",
    "        \n",
    "\n",
    "        # text_to_clean['text'] = ' '.join([slangs_dict[word.lower()] if word in slangs_dict.keys() else word for word in text_to_clean['text'].split])\n",
    "\n",
    "        step = 'attached'\n",
    "        # print(text_to_clean['text'])\n",
    "        # print('attached')\n",
    "\n",
    "        text_to_clean['text'] = ' '.join(re.findall('[A-Z][^A-Z]*', text_to_clean['text']))\n",
    "        \n",
    "        step = 'regex'\n",
    "        # print(text_to_clean['text'])\n",
    "        # print('regex')\n",
    "\n",
    "        text_to_clean['text'] = re.sub('@[^\\s]+','',text_to_clean['text'])\n",
    "        # text_to_clean['text'] = re.sub('#[^\\s]+','',text_to_clean['text'])\n",
    "        text_to_clean['text'] = re.sub('http[^\\s]+','',text_to_clean['text'])\n",
    "        text_to_clean['text'] = re.sub('[^A-Za-z ]+','',text_to_clean['text'])\n",
    "\n",
    "        step = 'spell'\n",
    "        # print(text_to_clean['text'])\n",
    "        # print('spell')\n",
    "\n",
    "        spell = Speller(lang='en')\n",
    "        text_to_clean['text'] =spell(text_to_clean['text'])\n",
    "\n",
    "        step = 'token'\n",
    "        # print(text_to_clean['text'])\n",
    "        # print('token')\n",
    "\n",
    "        text_to_clean['text'] = word_tokenize(text_to_clean['text'])\n",
    "\n",
    "        step = 'stop'\n",
    "        # print(text_to_clean['text'])\n",
    "        # print('stop')\n",
    "\n",
    "        text_to_clean['text'] = [word for word in text_to_clean['text'] if word not in stopwords.words('english')]\n",
    "\n",
    "        step = 'punct'\n",
    "        # print(text_to_clean['text'])\n",
    "        # print('punct')\n",
    "\n",
    "        text_to_clean['text'] = [word for word in text_to_clean['text'] if word not in punctuation_list]\n",
    "\n",
    "        step = 'lower'\n",
    "        # print(text_to_clean['text'])\n",
    "        # print('lower')\n",
    "\n",
    "        text_to_clean['text'] = [word.lower() for word in text_to_clean['text']]\n",
    "\n",
    "        # print(text_to_clean['text'])\n",
    "\n",
    "        \n",
    "        return  text_to_clean['text']\n",
    "    except Exception as e:\n",
    "        print(f'Failed at {step} with: {text_to_clean}, {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('./data//twitter_posts.csv', delimiter=',', names=['text'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ngo, ham, yr, old, vietnamese, man, one, asia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[i, making, following, announcement, formal, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the, aggressive, field, operation, california...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[thank, supervisor, aaron, skin, endorsing, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[wishing, luck, enters, final, week, race, ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>[unborn, children, welcomed, life, protected, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>[i, vote, john, bother, my, statement, tot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>[i, believe, warriors, like, chris, kyle, best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>[millions, americans, stand, israel, r, t, i, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>[i, congratulate, rep, ted, yoo, courageous, r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1392 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     [ngo, ham, yr, old, vietnamese, man, one, asia...\n",
       "1     [i, making, following, announcement, formal, a...\n",
       "2     [the, aggressive, field, operation, california...\n",
       "3     [thank, supervisor, aaron, skin, endorsing, se...\n",
       "4     [wishing, luck, enters, final, week, race, ind...\n",
       "...                                                 ...\n",
       "1823  [unborn, children, welcomed, life, protected, ...\n",
       "1824        [i, vote, john, bother, my, statement, tot]\n",
       "1825  [i, believe, warriors, like, chris, kyle, best...\n",
       "1826  [millions, americans, stand, israel, r, t, i, ...\n",
       "1827  [i, congratulate, rep, ted, yoo, courageous, r...\n",
       "\n",
       "[1392 rows x 1 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ~4 min\n",
    "tweets = clean_df(tweets)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "headers = {\"Authorization\": f\"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": [\"I like you. I love you\",\n",
    "\t    \"no hate, don't like\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'positive', 'score': 0.9299362301826477},\n",
       "  {'label': 'neutral', 'score': 0.06471794098615646},\n",
       "  {'label': 'negative', 'score': 0.0053458199836313725}],\n",
       " [{'label': 'negative', 'score': 0.6892377138137817},\n",
       "  {'label': 'neutral', 'score': 0.2863863408565521},\n",
       "  {'label': 'positive', 'score': 0.024375932291150093}]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
